"""
æœç´¢APIè·¯ç”± - V3å¢å¼ºç‰ˆ
æ”¯æŒæ··åˆæ£€ç´¢å’Œåˆ†è¯æ§åˆ¶
"""

import logging
import re
from typing import Optional, List
from fastapi import APIRouter, Query, HTTPException, Depends

from core.models import (
    SearchRequest, SearchResponse, IndexRequest, IndexResponse,
    TokenizationResult, HybridSearchConfig,
    MetricSearchRequest, MetricSearchResponse,
    ComprehensiveAnalysisRequest, ComprehensiveAnalysisResponse
)
from search.hybrid_searcher import HybridSearcher
from indexing.data_loader import MetadataLoader

logger = logging.getLogger(__name__)

router = APIRouter()

def remove_time_from_query(query: str) -> str:
    """
    ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†ï¼Œä¿ç•™å…¶ä»–å†…å®¹
    
    æ”¯æŒç§»é™¤çš„æ—¶é—´æ ¼å¼ï¼š
    - 2025-10-13
    - 2025-09-01è‡³2025-10-14
    - 2025-09-01åˆ°2025-10-14
    - 2025/10/13
    - 2025.10.13
    - 2025å¹´10æœˆ13æ—¥
    - 2025-10-13 10:30:00
    - 2025-10-13 10:30
    """
    # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²
    original_query = query
    query = query.strip()
    
    # æ—¶é—´èŒƒå›´æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼˜å…ˆåŒ¹é…èŒƒå›´ï¼Œå› ä¸ºå®ƒä»¬æ›´é•¿ï¼‰
    range_patterns = [
        # åŒ¹é… "2025-09-01 åˆ° 2025-09-30" è¿™ç§æ ¼å¼
        r'\d{4}-\d{1,2}-\d{1,2}\s*[è‡³åˆ°]\s*\d{4}-\d{1,2}-\d{1,2}',
        r'\d{4}/\d{1,2}/\d{1,2}\s*[è‡³åˆ°]\s*\d{4}/\d{1,2}/\d{1,2}',
        r'\d{4}\.\d{1,2}\.\d{1,2}\s*[è‡³åˆ°]\s*\d{4}\.\d{1,2}\.\d{1,2}',
        r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥\s*[è‡³åˆ°]\s*\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
    ]
    
    # ç§»é™¤æ—¶é—´èŒƒå›´
    for pattern in range_patterns:
        query = re.sub(pattern, '', query)
    
    # å•ç‹¬çš„æ—¶é—´æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    time_patterns = [
        # YYYY-MM-DD HH:MM:SS æ ¼å¼ï¼ˆå¸¦æ—¶é—´çš„è¦å…ˆåŒ¹é…ï¼‰
        r'\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{1,2}:\d{1,2}',
        r'\d{4}/\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2}:\d{1,2}',
        # YYYY-MM-DD HH:MM æ ¼å¼
        r'\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{1,2}',
        r'\d{4}/\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2}',
        # YYYY-MM-DD æ ¼å¼
        r'\d{4}-\d{1,2}-\d{1,2}',
        # YYYY/MM/DD æ ¼å¼
        r'\d{4}/\d{1,2}/\d{1,2}',
        # YYYY.MM.DD æ ¼å¼
        r'\d{4}\.\d{1,2}\.\d{1,2}',
        # YYYYå¹´MMæœˆDDæ—¥ æ ¼å¼
        r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
    ]
    
    # ç§»é™¤å•ç‹¬çš„æ—¶é—´æ ¼å¼
    for pattern in time_patterns:
        query = re.sub(pattern, '', query)
    
    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    query = re.sub(r'\s+', ' ', query).strip()
    
    # å¦‚æœå¤„ç†åçš„æŸ¥è¯¢ä¸ºç©ºæˆ–åªå‰©ä¸‹å¾ˆå°‘çš„å­—ç¬¦ï¼Œè¿”å›åŸæŸ¥è¯¢
    if len(query) < 2:
        return original_query
    
    # è®°å½•æ—¶é—´è¿‡æ»¤æ—¥å¿—
    if query != original_query:
        logger.info(f"ä»æŸ¥è¯¢ä¸­ç§»é™¤äº†æ—¶é—´éƒ¨åˆ†: '{original_query}' -> '{query}'")
    
    return query

# å…¨å±€æ··åˆæœç´¢å™¨å®ä¾‹
_hybrid_searcher = None
_initialization_attempted = False
_data_sync_scheduler = None

def get_hybrid_searcher() -> HybridSearcher:
    """è·å–æ··åˆæœç´¢å™¨å®ä¾‹ - æ”¯æŒè‡ªåŠ¨åˆå§‹åŒ–å’Œç´¢å¼•åˆ›å»º"""
    global _hybrid_searcher, _initialization_attempted, _data_sync_scheduler
    
    if _hybrid_searcher is None:
        logger.info("åˆ›å»ºæ··åˆæœç´¢å™¨å®ä¾‹...")
        _hybrid_searcher = HybridSearcher()
    
    # åªåœ¨ç¬¬ä¸€æ¬¡æˆ–è¢«æ˜¾å¼é‡ç½®åæ‰å°è¯•åˆå§‹åŒ–
    if not _initialization_attempted:
        _initialization_attempted = True
        logger.info("é¦–æ¬¡æ£€æµ‹åˆ°æœç´¢å™¨ï¼Œå¼€å§‹æ£€æŸ¥ç´¢å¼•çŠ¶æ€...")
        
        # å…ˆæ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²ç»å­˜åœ¨ä¸”æœ‰æ•°æ®
        need_initialization = True
        if _hybrid_searcher.es_engine:
            try:
                # æ£€æŸ¥å­—æ®µç´¢å¼•
                fields_exist = False
                fields_count = 0
                if _hybrid_searcher.es_engine.index_exists():
                    count_response = _hybrid_searcher.es_engine.es.count(
                        index=_hybrid_searcher.es_engine.fields_index_name
                    ) 
                    fields_count = count_response.get('count', 0)
                    fields_exist = fields_count > 0
                
                # æ£€æŸ¥æŒ‡æ ‡ç´¢å¼•
                metrics_exist = False
                metrics_count = 0
                if _hybrid_searcher.es_engine.metric_index_exists():
                    count_response = _hybrid_searcher.es_engine.es.count(
                        index=_hybrid_searcher.es_engine.metric_index_name
                    )
                    metrics_count = count_response.get('count', 0)
                    metrics_exist = metrics_count > 0
                
                # æ£€æŸ¥ç»´åº¦å€¼ç´¢å¼•
                dimension_values_exist = False
                dimension_values_count = 0
                if _hybrid_searcher.es_engine.dimension_values_index_exists():
                    count_response = _hybrid_searcher.es_engine.es.count(
                        index=_hybrid_searcher.es_engine.dimension_values_index_name
                    )
                    dimension_values_count = count_response.get('count', 0)
                    dimension_values_exist = dimension_values_count > 0
                
                # è¾“å‡ºç´¢å¼•çŠ¶æ€
                if fields_exist or metrics_exist or dimension_values_exist:
                    status_msg = []
                    if fields_exist:
                        status_msg.append(f"å­—æ®µç´¢å¼•({fields_count}æ¡)")
                    if metrics_exist:
                        status_msg.append(f"æŒ‡æ ‡ç´¢å¼•({metrics_count}æ¡)")
                    if dimension_values_exist:
                        status_msg.append(f"ç»´åº¦å€¼ç´¢å¼•({dimension_values_count}æ¡)")
                    logger.info(f"å‘ç°å·²å­˜åœ¨çš„ç´¢å¼•: {', '.join(status_msg)}")
                
                # åªæœ‰å½“ä¸‰ä¸ªç´¢å¼•éƒ½å­˜åœ¨ä¸”æœ‰æ•°æ®æ—¶ï¼Œæ‰è·³è¿‡åˆå§‹åŒ–
                if fields_exist and metrics_exist and dimension_values_exist:
                    logger.info("âœ… å­—æ®µç´¢å¼•ã€æŒ‡æ ‡ç´¢å¼•å’Œç»´åº¦å€¼ç´¢å¼•éƒ½å·²å­˜åœ¨ä¸”æœ‰æ•°æ®")
                    
                    # æ ‡è®°æœç´¢å™¨ä¸ºå·²åˆå§‹åŒ–
                    _hybrid_searcher.initialized = True
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–ACè‡ªåŠ¨æœºå’Œç›¸ä¼¼åº¦åŒ¹é…å™¨
                    need_other_engines = (
                        (_hybrid_searcher.ac_matcher and not _hybrid_searcher.ac_matcher.initialized) or
                        (_hybrid_searcher.similarity_matcher and not _hybrid_searcher.similarity_matcher.initialized)
                    )
                    
                    if need_other_engines:
                        # å½“ç´¢å¼•å·²å­˜åœ¨æ—¶ï¼ŒACè‡ªåŠ¨æœºå’Œç›¸ä¼¼åº¦åŒ¹é…å™¨å°†å»¶è¿Ÿåˆå§‹åŒ–
                        # é¿å…åœ¨å¯åŠ¨æ—¶é‡å¤ä»APIåŠ è½½æ•°æ®
                        logger.info("æ£€æµ‹åˆ°ç´¢å¼•å·²å­˜åœ¨ï¼ŒACè‡ªåŠ¨æœºå’Œç›¸ä¼¼åº¦åŒ¹é…å™¨å°†åœ¨é¦–æ¬¡æœç´¢æˆ–åŒæ­¥æ—¶åˆå§‹åŒ–")
                    else:
                        logger.info("âœ… ACè‡ªåŠ¨æœºå’Œç›¸ä¼¼åº¦åŒ¹é…å™¨å·²åˆå§‹åŒ–ï¼Œè·³è¿‡")
                    
                    need_initialization = False
                elif fields_exist and metrics_exist and not dimension_values_exist:
                    logger.info("âš ï¸ å­—æ®µå’ŒæŒ‡æ ‡ç´¢å¼•å­˜åœ¨ä½†ç»´åº¦å€¼ç´¢å¼•ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»ºç»´åº¦å€¼ç´¢å¼•")
                elif fields_exist and not metrics_exist:
                    logger.info("âš ï¸ å­—æ®µç´¢å¼•å­˜åœ¨ä½†æŒ‡æ ‡/ç»´åº¦å€¼ç´¢å¼•ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»ºç¼ºå¤±çš„ç´¢å¼•")
                elif not fields_exist and (metrics_exist or dimension_values_exist):
                    logger.info("âš ï¸ å­—æ®µç´¢å¼•ä¸å­˜åœ¨ä½†å…¶ä»–ç´¢å¼•å­˜åœ¨ï¼Œéœ€è¦å®Œæ•´é‡å»ºæ‰€æœ‰ç´¢å¼•")
                else:
                    logger.info("ç´¢å¼•ä¸å­˜åœ¨æˆ–æ— æ•°æ®ï¼Œéœ€è¦åˆ›å»ºç´¢å¼•å’ŒåŠ è½½æ•°æ®")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€æ—¶å‡ºé”™: {e}ï¼Œå°†å°è¯•åˆå§‹åŒ–")
        
        # åªæœ‰åœ¨çœŸæ­£éœ€è¦æ—¶æ‰è¿›è¡Œå®Œæ•´åˆå§‹åŒ–
        if need_initialization:
            logger.info("å¼€å§‹è‡ªåŠ¨åˆ›å»ºç´¢å¼•å’ŒåŠ è½½æ•°æ®...")
            try:
                result = _hybrid_searcher.create_index_with_data(
                    excel_path=None,
                    force_recreate=False
                )
                
                if result.get('success', False):
                    logger.info(f"âœ… è‡ªåŠ¨åˆ›å»ºç´¢å¼•æˆåŠŸ: {result.get('message', '')}")
                    logger.info(f"ğŸ“Š è€—æ—¶: {result.get('took', 0)}ms")
                    
                    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                    stats = result.get('stats', {})
                    if stats:
                        logger.info(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
                        logger.info(f"  - æ€»å­—æ®µæ•°: {stats.get('total_fields', 0)}")
                        logger.info(f"  - æœç´¢å™¨çŠ¶æ€: {'å·²åˆå§‹åŒ–' if stats.get('initialized', False) else 'æœªåˆå§‹åŒ–'}")
                        
                        engines = stats.get('engines', {})
                        for engine_name, engine_info in engines.items():
                            status = 'âœ… å¯ç”¨' if engine_info.get('available', False) else 'âŒ ä¸å¯ç”¨'
                            logger.info(f"  - {engine_name}: {status}")
                else:
                    logger.error(f"âŒ è‡ªåŠ¨åˆ›å»ºç´¢å¼•å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨åˆ›å»ºç´¢å¼•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    # å¯åŠ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if _data_sync_scheduler is None:
        from core.config import config
        if config.API_SYNC_ENABLED:
            try:
                from indexing.scheduler import DataSyncScheduler
                logger.info("åˆå§‹åŒ–æ•°æ®åŒæ­¥è°ƒåº¦å™¨...")
                _data_sync_scheduler = DataSyncScheduler(_hybrid_searcher)
                _data_sync_scheduler.start()
            except Exception as e:
                logger.error(f"å¯åŠ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å¤±è´¥: {e}")
    
    return _hybrid_searcher

def ensure_searcher_ready(searcher: HybridSearcher) -> bool:
    """ç¡®ä¿æœç´¢å™¨å·²å‡†å¤‡å°±ç»ªï¼ˆå·²åˆå§‹åŒ–ä¸”æœ‰æ•°æ®ï¼‰"""
    if not searcher.initialized:
        logger.warning("æœç´¢å™¨æœªåˆå§‹åŒ–ï¼Œæ£€æŸ¥ç´¢å¼•çŠ¶æ€...")
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å®é™…å­˜åœ¨ä¸”æœ‰æ•°æ®
        if searcher.es_engine:
            try:
                if searcher.es_engine.index_exists():
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•°æ®
                    count_response = searcher.es_engine.es.count(index=searcher.es_engine.fields_index_name)
                    existing_count = count_response.get('count', 0)
                    
                    if existing_count > 0:
                        logger.info(f"å‘ç°ç´¢å¼•å·²å­˜åœ¨ä¸”æœ‰ {existing_count} æ¡æ•°æ®ï¼Œæ ‡è®°æœç´¢å™¨ä¸ºå·²åˆå§‹åŒ–")
                        searcher.initialized = True
                        return True
                    else:
                        logger.info("ç´¢å¼•å­˜åœ¨ä½†æ— æ•°æ®ï¼Œéœ€è¦é‡æ–°åŠ è½½æ•°æ®")
                else:
                    logger.info("ç´¢å¼•ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»ºç´¢å¼•")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€æ—¶å‡ºé”™: {e}")
        
        # å¦‚æœç¡®å®éœ€è¦åˆå§‹åŒ–ï¼Œåˆ™è¿›è¡Œåˆå§‹åŒ–
        logger.warning("å°è¯•é‡æ–°åˆå§‹åŒ–æœç´¢å™¨...")
        try:
            # ç›´æ¥è°ƒç”¨ create_index_with_data æ¥é‡æ–°åˆ›å»º
            result = searcher.create_index_with_data(
                excel_path=None,
                force_recreate=False
            )
            
            if result.get('success', False):
                logger.info(f"âœ… æœç´¢å™¨é‡æ–°åˆå§‹åŒ–æˆåŠŸ: {result.get('message', '')}")
                return True
            else:
                logger.error(f"âŒ æœç´¢å™¨é‡æ–°åˆå§‹åŒ–å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        except Exception as e:
            logger.error(f"âŒ é‡æ–°åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False
    return True


@router.get("/fields", response_model=SearchResponse, summary="æœç´¢å…ƒæ•°æ®å­—æ®µ")
async def search_fields(
    q: str = Query(..., description="æœç´¢æŸ¥è¯¢"),
    table_name: Optional[List[str]] = Query(None, description="é™åˆ¶æœç´¢çš„è¡¨ååˆ—è¡¨ï¼Œæ”¯æŒå¤šè¡¨é€‰æ‹©"),
    enabled_only: bool = Query(True, description="ä»…æœç´¢å¯ç”¨å­—æ®µ"),
    size: int = Query(10, ge=1, le=100, description="è¿”å›ç»“æœæ•°é‡"),
    use_tokenization: bool = Query(True, description="æ˜¯å¦å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯å¤„ç†"),
    tokenizer_type: str = Query("ik_max_word", description="åˆ†è¯å™¨ç±»å‹ï¼šik_smart/ik_max_word/standard"),
    search_method: str = Query("hybrid", description="æœç´¢æ–¹æ³•ï¼šhybrid/elasticsearch/ac_matcher/similarity"),
    highlight: bool = Query(True, description="æ˜¯å¦è¿”å›é«˜äº®ä¿¡æ¯"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æœç´¢å…ƒæ•°æ®å­—æ®µ - æ”¯æŒæ··åˆæ£€ç´¢å’Œåˆ†è¯æ§åˆ¶
    
    ## æœç´¢æ–¹æ³•è¯´æ˜
    - **hybrid**: æ··åˆæœç´¢ï¼Œç»“åˆå¤šç§æœç´¢ç®—æ³•
    - **elasticsearch**: ä»…ä½¿ç”¨Elasticsearchå…¨æ–‡æœç´¢
    - **ac_matcher**: ä»…ä½¿ç”¨ACè‡ªåŠ¨æœºç²¾ç¡®åŒ¹é…
    - **similarity**: ä»…ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…
    
    ## åˆ†è¯æ§åˆ¶è¯´æ˜
    - **use_tokenization=true**: å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œé€‚åˆé•¿æ–‡æœ¬å’Œå¤æ‚æŸ¥è¯¢
    - **use_tokenization=false**: ä¸åˆ†è¯ï¼Œè¿›è¡Œç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆä¸“ä¸šæœ¯è¯­æœç´¢
    - **tokenizer_type**: åˆ†è¯å™¨ç±»å‹ï¼Œä»…åœ¨use_tokenization=trueæ—¶ç”Ÿæ•ˆ
    
    ## å¤šè¡¨é€‰æ‹©è¯´æ˜
    - **å•è¡¨**: `?table_name=ç”¨æˆ·è¡¨`
    - **å¤šè¡¨**: `?table_name=ç”¨æˆ·è¡¨&table_name=å®¢æˆ·è¡¨&table_name=è®¢å•è¡¨`
    """
    try:
        logger.info(f"æœç´¢è¯·æ±‚: query='{q}', method='{search_method}', tokenization={use_tokenization}")
        
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        cleaned_query = remove_time_from_query(q)
        
        # ç¡®ä¿æœç´¢å™¨å·²å‡†å¤‡å°±ç»ª
        if not ensure_searcher_ready(searcher):
            raise HTTPException(
                status_code=503, 
                detail="æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        
        # åˆ›å»ºæœç´¢è¯·æ±‚
        request = SearchRequest(
            query=cleaned_query,
            table_name=table_name,
            enabled_only=enabled_only,
            size=size,
            use_tokenization=use_tokenization,
            tokenizer_type=tokenizer_type,
            search_method=search_method,
            highlight=highlight
        )
        
        # æ‰§è¡Œæœç´¢
        response = searcher.search(request)
        
        logger.info(f"æœç´¢å®Œæˆ: æ‰¾åˆ° {response.total} ä¸ªç»“æœï¼Œè€—æ—¶ {response.took}msï¼Œä½¿ç”¨æ–¹æ³• {response.search_methods}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/fields", response_model=SearchResponse, summary="POSTæ–¹å¼æœç´¢å­—æ®µ")
async def search_fields_post(
    request: SearchRequest,
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    POSTæ–¹å¼æœç´¢å­—æ®µï¼ˆæ”¯æŒå¤æ‚æŸ¥è¯¢å‚æ•°ï¼‰
    """
    try:
        logger.info(f"POSTæœç´¢è¯·æ±‚: {request.model_dump()}")
        
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        request.query = remove_time_from_query(request.query)
        
        # ç¡®ä¿æœç´¢å™¨å·²å‡†å¤‡å°±ç»ª
        if not ensure_searcher_ready(searcher):
            raise HTTPException(
                status_code=503, 
                detail="æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        
        response = searcher.search(request)
        
        logger.info(f"POSTæœç´¢å®Œæˆ: æ‰¾åˆ° {response.total} ä¸ªç»“æœï¼Œè€—æ—¶ {response.took}ms")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"POSTæœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.get("/tokenize", response_model=TokenizationResult, summary="æ–‡æœ¬åˆ†è¯")
async def tokenize_text(
    text: str = Query(..., description="å¾…åˆ†è¯æ–‡æœ¬"),
    tokenizer_type: str = Query("ik_max_word", description="åˆ†è¯å™¨ç±»å‹"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
    
    ## åˆ†è¯å™¨ç±»å‹
    - **ik_max_word**: IKæœ€å¤§è¯é•¿åˆ†è¯å™¨ï¼ˆæ¨èï¼‰
    - **ik_smart**: IKæ™ºèƒ½åˆ†è¯å™¨
    - **standard**: æ ‡å‡†åˆ†è¯å™¨
    """
    try:
        if not searcher.es_engine:
            raise HTTPException(status_code=503, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        result = searcher.es_engine.tokenize_text(text, tokenizer_type)
        return result
        
    except Exception as e:
        logger.error(f"åˆ†è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†è¯å¤±è´¥: {str(e)}")


@router.get("/suggest", summary="æœç´¢å»ºè®®")
async def get_search_suggestions(
    q: str = Query(..., description="æœç´¢æŸ¥è¯¢"),
    size: int = Query(5, ge=1, le=20, description="å»ºè®®æ•°é‡"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    è·å–æœç´¢å»ºè®®
    """
    try:
        # ç¡®ä¿æœç´¢å™¨å·²å‡†å¤‡å°±ç»ª
        if not ensure_searcher_ready(searcher):
            raise HTTPException(
                status_code=503, 
                detail="æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        
        # ä½¿ç”¨æ··åˆæœç´¢è·å–å»ºè®®
        request = SearchRequest(
            query=q,
            size=size,
            search_method="hybrid",
            use_tokenization=True
        )
        
        response = searcher.search(request)
        
        suggestions = []
        for result in response.results:
            field = result.field
            suggestions.append({
                "text": field.chinese_name,
                "value": field.column_name,
                "table": field.table_name,
                "score": result.score,
                "search_method": result.search_method
            })
        
        return {
            "query": q,
            "suggestions": suggestions,
            "took": response.took,
            "search_methods": response.search_methods
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æœç´¢å»ºè®®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å»ºè®®å¤±è´¥: {str(e)}")


@router.get("/tables", summary="è·å–è¡¨åˆ—è¡¨")
async def get_tables(
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    è·å–æ‰€æœ‰è¡¨çš„åˆ—è¡¨
    """
    try:
        if not searcher.es_engine or not searcher.es_engine.index_exists():
            raise HTTPException(status_code=404, detail="ç´¢å¼•ä¸å­˜åœ¨")
        
        # ä½¿ç”¨ElasticsearchèšåˆæŸ¥è¯¢è·å–è¡¨åˆ—è¡¨
        es = searcher.es_engine.es
        response = es.search(
            index=searcher.es_engine.index_name,
            body={
                "size": 0,
                "aggs": {
                    "tables": {
                        "terms": {
                            "field": "table_name",
                            "size": 1000
                        }
                    }
                }
            }
        )
        
        tables = []
        for bucket in response['aggregations']['tables']['buckets']:
            tables.append({
                "name": bucket['key'],
                "count": bucket['doc_count']
            })
        
        return {
            "tables": tables,
            "total": len(tables)
        }
        
    except Exception as e:
        logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/extract", summary="æå–å®ä½“")
async def extract_entities(
    text: str = Query(..., description="å¾…æå–å®ä½“çš„æ–‡æœ¬"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    ä»æ–‡æœ¬ä¸­æå–å®ä½“
    """
    try:
        # ç¡®ä¿æœç´¢å™¨å·²å‡†å¤‡å°±ç»ª
        if not ensure_searcher_ready(searcher):
            raise HTTPException(
                status_code=503, 
                detail="æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        
        entities = searcher.extract_entities(text)
        
        return {
            "text": text,
            "entities": entities,
            "count": len(entities)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®ä½“æå–å¤±è´¥: {str(e)}")


@router.post("/index/create", response_model=IndexResponse, summary="åˆ›å»ºç´¢å¼•å¹¶åŠ è½½æ•°æ®")
async def create_index_with_data(
    request: IndexRequest,
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æ‰‹åŠ¨åˆ›å»ºç´¢å¼•å¹¶è‡ªåŠ¨åŠ è½½æ•°æ®
    
    ## åŠŸèƒ½è¯´æ˜
    - åˆ›å»ºElasticsearchç´¢å¼•ï¼ˆå…ƒæ•°æ®å­—æ®µç´¢å¼• + æŒ‡æ ‡ç´¢å¼•ï¼‰
    - ä»Excelæ–‡ä»¶åŠ è½½å…ƒæ•°æ®å’ŒæŒ‡æ ‡æ•°æ®
    - åˆå§‹åŒ–æ‰€æœ‰æœç´¢å¼•æ“ï¼ˆESã€ACè‡ªåŠ¨æœºã€ç›¸ä¼¼åº¦åŒ¹é…å™¨ï¼‰
    - è¿”å›è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    
    æ³¨æ„ï¼šç³»ç»Ÿä¼šåœ¨é¦–æ¬¡è®¿é—®æ—¶è‡ªåŠ¨åˆå§‹åŒ–ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨æ­¤æ¥å£
    """
    try:
        logger.info(f"æ‰‹åŠ¨åˆ›å»ºç´¢å¼•è¯·æ±‚: {request.model_dump()}")
        
        # åˆ›å»ºå…ƒæ•°æ®å­—æ®µç´¢å¼•
        result = searcher.create_index_with_data(
            excel_path=request.excel_path,
            force_recreate=request.force_recreate
        )
        
        # åŒæ—¶åˆ›å»ºæŒ‡æ ‡ç´¢å¼•
        metric_result = None
        try:
            logger.info("åŒæ—¶åˆ›å»ºæŒ‡æ ‡ç´¢å¼•...")
            metric_result = searcher.create_and_load_metrics(
                force_recreate=request.force_recreate
            )
            
            if metric_result['success']:
                logger.info(f"âœ… æŒ‡æ ‡ç´¢å¼•åˆ›å»ºæˆåŠŸ: {metric_result['message']}")
                # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
                if result.get('stats'):
                    result['stats']['metric_indexing'] = metric_result.get('stats', {})
                else:
                    result['stats'] = {'metric_indexing': metric_result.get('stats', {})}
                
                # æ›´æ–°æ¶ˆæ¯
                result['message'] = f"{result['message']}ï¼›{metric_result['message']}"
            else:
                logger.warning(f"æŒ‡æ ‡ç´¢å¼•åˆ›å»ºå¤±è´¥: {metric_result['message']}")
                
        except Exception as me:
            logger.warning(f"æŒ‡æ ‡ç´¢å¼•åˆ›å»ºå‡ºé”™ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {me}")
        
        # é‡ç½®åˆå§‹åŒ–æ ‡å¿—ï¼Œç¡®ä¿ä¸‹æ¬¡æ£€æŸ¥æ—¶èƒ½è·å¾—æœ€æ–°çŠ¶æ€
        global _initialization_attempted
        _initialization_attempted = False
        
        return IndexResponse(
            success=result['success'],
            message=result['message'],
            stats=result.get('stats'),
            took=result['took']
        )
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        return IndexResponse(
            success=False,
            message=f"åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}",
            took=0
        )


@router.get("/stats", summary="è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
async def get_system_stats(
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    """
    try:
        stats = searcher.get_stats()
        return stats
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/health", summary="å¥åº·æ£€æŸ¥")
async def health_check(
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    ç³»ç»Ÿå¥åº·æ£€æŸ¥
    """
    try:
        health_status = {
            "status": "healthy",
            "timestamp": "2024-01-01T00:00:00Z",
            "services": {}
        }
        
        # æ£€æŸ¥å„ä¸ªæœåŠ¡çŠ¶æ€
        if searcher.es_engine:
            try:
                health_status["services"]["elasticsearch"] = {
                    "status": "healthy" if searcher.es_engine.index_exists() else "index_missing",
                    "index_exists": searcher.es_engine.index_exists()
                }
            except Exception as e:
                health_status["services"]["elasticsearch"] = {
                    "status": "unhealthy",
                    "error": str(e)
                }
        
        health_status["services"]["ac_matcher"] = {
            "status": "healthy" if (searcher.ac_matcher and searcher.ac_matcher.initialized) else "not_initialized"
        }
        
        health_status["services"]["similarity"] = {
            "status": "healthy" if (searcher.similarity_matcher and searcher.similarity_matcher.initialized) else "not_initialized"
        }
        
        # åˆ¤æ–­æ•´ä½“çŠ¶æ€
        unhealthy_services = [k for k, v in health_status["services"].items() if v["status"] != "healthy"]
        if unhealthy_services:
            health_status["status"] = "degraded"
            health_status["issues"] = unhealthy_services
        
        return health_status
        
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": "2024-01-01T00:00:00Z"
        }


@router.get("/dimension-values", response_model=SearchResponse, 
            summary="æœç´¢ç»´åº¦å€¼", description="åœ¨ç»´åº¦å€¼ç´¢å¼•ä¸­æœç´¢ç‰¹å®šçš„ç»´åº¦å€¼")
async def search_dimension_values(
    q: str = Query(..., description="æœç´¢æŸ¥è¯¢"),
    table_name: Optional[List[str]] = Query(None, description="é™åˆ¶æœç´¢çš„è¡¨ååˆ—è¡¨ï¼Œæ”¯æŒå¤šè¡¨é€‰æ‹©"),
    column_name: Optional[str] = Query(None, description="é™åˆ¶æœç´¢çš„åˆ—å"),
    size: int = Query(10, ge=1, le=100, description="è¿”å›ç»“æœæ•°é‡"),
    use_tokenization: bool = Query(True, description="æ˜¯å¦ä½¿ç”¨åˆ†è¯"),
    tokenizer_type: str = Query("ik_max_word", description="åˆ†è¯å™¨ç±»å‹"),
    highlight: bool = Query(True, description="æ˜¯å¦è¿”å›é«˜äº®ä¿¡æ¯")
):
    """
    æœç´¢ç»´åº¦å€¼ - åœ¨ç»´åº¦å€¼ç´¢å¼•ä¸­æŸ¥æ‰¾åŒ¹é…çš„ç»´åº¦å€¼
    
    ## å¤šè¡¨é€‰æ‹©è¯´æ˜
    - **å•è¡¨**: `?table_name=ç”¨æˆ·è¡¨`
    - **å¤šè¡¨**: `?table_name=ç”¨æˆ·è¡¨&table_name=å®¢æˆ·è¡¨&table_name=è®¢å•è¡¨`
    - **ä¸é™åˆ¶è¡¨**: ä¸ä¼  table_name å‚æ•°
    """
    try:
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        cleaned_query = remove_time_from_query(q)
        
        searcher = get_hybrid_searcher()
        
        # æ„å»ºæœç´¢è¯·æ±‚
        request = SearchRequest(
            query=cleaned_query,
            table_name=table_name,
            size=size,
            use_tokenization=use_tokenization,
            tokenizer_type=tokenizer_type,
            search_method="dimension_values",
            highlight=highlight
        )
        
        # æ‰§è¡Œç»´åº¦å€¼æœç´¢
        response = searcher.search(request)
        
        return response
        
    except Exception as e:
        logger.error(f"ç»´åº¦å€¼æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç»´åº¦å€¼æœç´¢å¤±è´¥: {str(e)}")


@router.post("/dimension-values", response_model=SearchResponse,
             summary="ç»´åº¦å€¼POSTæœç´¢", description="ä½¿ç”¨POSTæ–¹æ³•è¿›è¡Œç»´åº¦å€¼æœç´¢ï¼Œæ”¯æŒæ›´å¤æ‚çš„æŸ¥è¯¢å‚æ•°")
async def search_dimension_values_post(request: SearchRequest):
    """ç»´åº¦å€¼POSTæœç´¢ - æ”¯æŒå¤æ‚çš„æœç´¢å‚æ•°"""
    try:
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        request.query = remove_time_from_query(request.query)
        
        searcher = get_hybrid_searcher()
        
        # å¼ºåˆ¶è®¾ç½®æœç´¢æ–¹æ³•ä¸ºç»´åº¦å€¼æœç´¢
        request.search_method = "dimension_values"
        
        response = searcher.search(request)
        return response
        
    except Exception as e:
        logger.error(f"ç»´åº¦å€¼æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç»´åº¦å€¼æœç´¢å¤±è´¥: {str(e)}")


@router.get("/database/test", 
            summary="æµ‹è¯•æ•°æ®åº“è¿æ¥", description="æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸")
async def test_database_connections():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        from indexing.dimension_extractor import EnhancedDimensionExtractor
        
        extractor = EnhancedDimensionExtractor()
        results = extractor.test_connections()
        extractor.close_connections()
        
        return {
            "success": True,
            "connections": results,
            "total_connections": len(results),
            "healthy_connections": len([r for r in results.values() if r.get('connected', False)])
        }
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")

@router.post("/dimension/extract", 
             summary="æ‰‹åŠ¨æå–ç»´åº¦å€¼", description="æ‰‹åŠ¨è§¦å‘ç»´åº¦å€¼æå–å’Œç´¢å¼•æ„å»º")
async def extract_dimension_values(force_recreate: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶é‡å»ºç»´åº¦å€¼ç´¢å¼•")):
    """æ‰‹åŠ¨æå–ç»´åº¦å€¼å¹¶æ„å»ºç´¢å¼•"""
    try:
        from indexing.data_loader import MetadataLoader
        from indexing.dimension_extractor import EnhancedDimensionExtractor
        
        searcher = get_hybrid_searcher()
        
        if not searcher.es_engine:
            raise HTTPException(status_code=500, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        # åŠ è½½å…ƒæ•°æ®
        loader = MetadataLoader()
        fields = loader.load()
        
        # åˆ›å»ºç»´åº¦å€¼ç´¢å¼•
        dimension_index_created = searcher.es_engine.create_dimension_values_index(force_recreate)
        if not dimension_index_created:
            raise HTTPException(status_code=500, detail="ç»´åº¦å€¼ç´¢å¼•åˆ›å»ºå¤±è´¥")
        
        # æå–ç»´åº¦å€¼
        extractor = EnhancedDimensionExtractor()
        dimension_values = extractor.extract_all_dimension_values(fields)
        
        if not dimension_values:
            extractor.close_connections()
            return {
                "success": True,
                "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦æå–çš„ç»´åº¦å€¼",
                "stats": {
                    "dimension_values_extracted": 0,
                    "dimension_values_indexed": 0
                }
            }
        
        # æ‰¹é‡ç´¢å¼•ç»´åº¦å€¼
        index_result = searcher.es_engine.bulk_index_dimension_values(dimension_values, force=force_recreate)
        extractor.close_connections()
        
        return {
            "success": True,
            "message": f"æˆåŠŸæå–å¹¶ç´¢å¼•äº† {index_result.get('success', 0)} ä¸ªç»´åº¦å€¼",
            "stats": {
                "dimension_values_extracted": len(dimension_values),
                "dimension_values_indexed": index_result.get('success', 0),
                "dimension_index_failed": index_result.get('failed', 0)
            }
        }
        
    except Exception as e:
        logger.error(f"ç»´åº¦å€¼æå–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç»´åº¦å€¼æå–å¤±è´¥: {str(e)}")


# ==================== Metricï¼ˆæŒ‡æ ‡ï¼‰ç›¸å…³API ====================

@router.get("/metrics",
            response_model=MetricSearchResponse,
            summary="æœç´¢æŒ‡æ ‡ï¼ˆGETï¼‰",
            description="é€šè¿‡GETæ–¹å¼æœç´¢æŒ‡æ ‡ï¼Œæ”¯æŒæŒ‰åç§°ã€åˆ«åã€ä¸šåŠ¡å®šä¹‰ç­‰æœç´¢")
async def search_metrics_get(
    q: str = Query(..., description="æœç´¢æŸ¥è¯¢å…³é”®è¯"),
    size: int = Query(10, ge=1, le=100, description="è¿”å›ç»“æœæ•°é‡"),
    use_tokenization: bool = Query(True, description="æ˜¯å¦ä½¿ç”¨åˆ†è¯"),
    tokenizer_type: str = Query("ik_max_word", description="åˆ†è¯å™¨ç±»å‹"),
    highlight: bool = Query(True, description="æ˜¯å¦è¿”å›é«˜äº®"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æœç´¢æŒ‡æ ‡ï¼ˆGETæ–¹å¼ï¼‰
    
    æ”¯æŒçš„æœç´¢å­—æ®µï¼š
    - metric_name: æŒ‡æ ‡åç§°
    - metric_alias: æŒ‡æ ‡åˆ«å
    - related_entities: ç›¸å…³å®ä½“
    
    ç¤ºä¾‹ï¼š
    - /api/search/metrics?q=æ‹œè®¿æ¬¡æ•°
    - /api/search/metrics?q=é”€å”®é¢&size=20
    """
    try:
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        cleaned_query = remove_time_from_query(q)
        
        # æ„å»ºæœç´¢è¯·æ±‚
        request = MetricSearchRequest(
            query=cleaned_query,
            size=size,
            use_tokenization=use_tokenization,
            tokenizer_type=tokenizer_type,
            highlight=highlight
        )
        
        # æ‰§è¡Œæœç´¢
        response = searcher.search_metrics(request)
        return response
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŒ‡æ ‡æœç´¢å¤±è´¥: {str(e)}")


@router.post("/metrics",
             response_model=MetricSearchResponse,
             summary="æœç´¢æŒ‡æ ‡ï¼ˆPOSTï¼‰",
             description="é€šè¿‡POSTæ–¹å¼æœç´¢æŒ‡æ ‡ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢å‚æ•°")
async def search_metrics_post(
    request: MetricSearchRequest,
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æœç´¢æŒ‡æ ‡ï¼ˆPOSTæ–¹å¼ï¼‰
    
    æ”¯æŒæ›´å¤æ‚çš„æŸ¥è¯¢å‚æ•°é…ç½®
    """
    try:
        # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ—¶é—´éƒ¨åˆ†
        request.query = remove_time_from_query(request.query)
        
        response = searcher.search_metrics(request)
        return response
        
    except Exception as e:
        logger.error(f"æŒ‡æ ‡æœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŒ‡æ ‡æœç´¢å¤±è´¥: {str(e)}")


# ==================== ç´¢å¼•ç®¡ç†API ====================

@router.delete("/index/delete",
               summary="åˆ é™¤ç´¢å¼•",
               description="åˆ é™¤æŒ‡å®šçš„ç´¢å¼•æˆ–æ‰€æœ‰ç´¢å¼•ï¼ˆè°¨æ…æ“ä½œï¼‰")
async def delete_indices(
    delete_fields_index: bool = Query(True, description="æ˜¯å¦åˆ é™¤å…ƒæ•°æ®å­—æ®µç´¢å¼•"),
    delete_dimension_values_index: bool = Query(True, description="æ˜¯å¦åˆ é™¤ç»´åº¦å€¼ç´¢å¼•"),
    delete_metrics_index: bool = Query(True, description="æ˜¯å¦åˆ é™¤æŒ‡æ ‡ç´¢å¼•"),
    confirm: bool = Query(False, description="ç¡®è®¤åˆ é™¤ï¼ˆå¿…é¡»è®¾ç½®ä¸ºtrueæ‰èƒ½æ‰§è¡Œåˆ é™¤ï¼‰"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    åˆ é™¤ç´¢å¼•
    
    **âš ï¸ è­¦å‘Š**: è¿™æ˜¯ä¸€ä¸ªå±é™©æ“ä½œï¼Œä¼šæ°¸ä¹…åˆ é™¤ç´¢å¼•æ•°æ®ï¼
    
    ## å‚æ•°è¯´æ˜
    - delete_fields_index: æ˜¯å¦åˆ é™¤å…ƒæ•°æ®å­—æ®µç´¢å¼•
    - delete_dimension_values_index: æ˜¯å¦åˆ é™¤ç»´åº¦å€¼ç´¢å¼•
    - delete_metrics_index: æ˜¯å¦åˆ é™¤æŒ‡æ ‡ç´¢å¼•
    - confirm: å¿…é¡»è®¾ç½®ä¸ºtrueæ‰èƒ½æ‰§è¡Œåˆ é™¤æ“ä½œ
    
    ## ç¤ºä¾‹
    ```
    # åˆ é™¤æ‰€æœ‰ç´¢å¼•
    DELETE /api/search/index/delete?confirm=true
    
    # åªåˆ é™¤æŒ‡æ ‡ç´¢å¼•
    DELETE /api/search/index/delete?delete_fields_index=false&delete_dimension_values_index=false&delete_metrics_index=true&confirm=true
    ```
    """
    try:
        # å®‰å…¨æ£€æŸ¥ï¼šå¿…é¡»æ˜ç¡®ç¡®è®¤æ‰èƒ½åˆ é™¤
        if not confirm:
            raise HTTPException(
                status_code=400, 
                detail="å¿…é¡»è®¾ç½® confirm=true å‚æ•°æ‰èƒ½æ‰§è¡Œåˆ é™¤æ“ä½œ"
            )
        
        if not searcher.es_engine:
            raise HTTPException(status_code=500, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        results = {
            "deleted_indices": [],
            "failed_indices": [],
            "skipped_indices": []
        }
        
        # åˆ é™¤å…ƒæ•°æ®å­—æ®µç´¢å¼•
        if delete_fields_index:
            try:
                index_name = searcher.es_engine.fields_index_name
                if searcher.es_engine.index_exists(index_name):
                    searcher.es_engine.es.indices.delete(index=index_name)
                    results["deleted_indices"].append({
                        "name": index_name,
                        "type": "å…ƒæ•°æ®å­—æ®µç´¢å¼•"
                    })
                    logger.info(f"âœ… å·²åˆ é™¤å…ƒæ•°æ®å­—æ®µç´¢å¼•: {index_name}")
                else:
                    results["skipped_indices"].append({
                        "name": index_name,
                        "type": "å…ƒæ•°æ®å­—æ®µç´¢å¼•",
                        "reason": "ç´¢å¼•ä¸å­˜åœ¨"
                    })
            except Exception as e:
                results["failed_indices"].append({
                    "name": index_name,
                    "type": "å…ƒæ•°æ®å­—æ®µç´¢å¼•",
                    "error": str(e)
                })
                logger.error(f"âŒ åˆ é™¤å…ƒæ•°æ®å­—æ®µç´¢å¼•å¤±è´¥: {e}")
        
        # åˆ é™¤ç»´åº¦å€¼ç´¢å¼•
        if delete_dimension_values_index:
            try:
                index_name = searcher.es_engine.dimension_values_index_name
                if searcher.es_engine.index_exists(index_name):
                    searcher.es_engine.es.indices.delete(index=index_name)
                    results["deleted_indices"].append({
                        "name": index_name,
                        "type": "ç»´åº¦å€¼ç´¢å¼•"
                    })
                    logger.info(f"âœ… å·²åˆ é™¤ç»´åº¦å€¼ç´¢å¼•: {index_name}")
                else:
                    results["skipped_indices"].append({
                        "name": index_name,
                        "type": "ç»´åº¦å€¼ç´¢å¼•",
                        "reason": "ç´¢å¼•ä¸å­˜åœ¨"
                    })
            except Exception as e:
                results["failed_indices"].append({
                    "name": index_name,
                    "type": "ç»´åº¦å€¼ç´¢å¼•",
                    "error": str(e)
                })
                logger.error(f"âŒ åˆ é™¤ç»´åº¦å€¼ç´¢å¼•å¤±è´¥: {e}")
        
        # åˆ é™¤æŒ‡æ ‡ç´¢å¼•
        if delete_metrics_index:
            try:
                index_name = searcher.es_engine.metric_index_name
                if searcher.es_engine.index_exists(index_name):
                    searcher.es_engine.es.indices.delete(index=index_name)
                    results["deleted_indices"].append({
                        "name": index_name,
                        "type": "æŒ‡æ ‡ç´¢å¼•"
                    })
                    logger.info(f"âœ… å·²åˆ é™¤æŒ‡æ ‡ç´¢å¼•: {index_name}")
                else:
                    results["skipped_indices"].append({
                        "name": index_name,
                        "type": "æŒ‡æ ‡ç´¢å¼•",
                        "reason": "ç´¢å¼•ä¸å­˜åœ¨"
                    })
            except Exception as e:
                results["failed_indices"].append({
                    "name": index_name,
                    "type": "æŒ‡æ ‡ç´¢å¼•",
                    "error": str(e)
                })
                logger.error(f"âŒ åˆ é™¤æŒ‡æ ‡ç´¢å¼•å¤±è´¥: {e}")
        
        # å¦‚æœåˆ é™¤äº†ç´¢å¼•ï¼Œé‡ç½®æœç´¢å™¨çš„åˆå§‹åŒ–çŠ¶æ€
        if results["deleted_indices"]:
            searcher.initialized = False
            global _initialization_attempted
            _initialization_attempted = False
            logger.info("å·²é‡ç½®æœç´¢å™¨åˆå§‹åŒ–çŠ¶æ€")
        
        # æ„å»ºå“åº”æ¶ˆæ¯
        success_count = len(results["deleted_indices"])
        failed_count = len(results["failed_indices"])
        skipped_count = len(results["skipped_indices"])
        
        message_parts = []
        if success_count > 0:
            message_parts.append(f"æˆåŠŸåˆ é™¤ {success_count} ä¸ªç´¢å¼•")
        if failed_count > 0:
            message_parts.append(f"åˆ é™¤å¤±è´¥ {failed_count} ä¸ª")
        if skipped_count > 0:
            message_parts.append(f"è·³è¿‡ {skipped_count} ä¸ªï¼ˆä¸å­˜åœ¨ï¼‰")
        
        message = "ï¼›".join(message_parts) if message_parts else "æ²¡æœ‰æ‰§è¡Œä»»ä½•åˆ é™¤æ“ä½œ"
        
        return {
            "success": failed_count == 0,
            "message": message,
            "results": results,
            "summary": {
                "total_requested": sum([delete_fields_index, delete_dimension_values_index, delete_metrics_index]),
                "deleted": success_count,
                "failed": failed_count,
                "skipped": skipped_count
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤ç´¢å¼•æ“ä½œå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ç´¢å¼•å¤±è´¥: {str(e)}")


# ==================== æ•°æ®åŒæ­¥API ====================

@router.post("/sync/metadata",
             summary="æ‰‹åŠ¨è§¦å‘å…ƒæ•°æ®åŒæ­¥",
             description="ä»APIæ‰‹åŠ¨åŒæ­¥å…ƒæ•°æ®åˆ°Elasticsearch")
async def sync_metadata_from_api(
    table_ids: Optional[List[int]] = Query(None, description="è¡¨IDåˆ—è¡¨ï¼Œç•™ç©ºåˆ™ä½¿ç”¨é…ç½®çš„è¡¨ID"),
    jwt: Optional[str] = Query(None, description="JWTè®¤è¯tokenï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„JWT"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æ‰‹åŠ¨ä»APIåŒæ­¥å…ƒæ•°æ®åˆ°ES
    
    ## å‚æ•°
    - table_ids: å¯é€‰ï¼ŒæŒ‡å®šè¦åŒæ­¥çš„è¡¨IDåˆ—è¡¨
    
    ## æ³¨æ„
    - å¦‚æœä¸æä¾›table_idsï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡API_TABLE_IDSé…ç½®çš„è¡¨ID
    - åŒæ­¥è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    - åŒæ­¥å®Œæˆåä¼šè‡ªåŠ¨æ›´æ–°æœç´¢å¼•æ“
    """
    try:
        from indexing.data_loader import MetadataLoader
        from core.config import config
        
        # ç¡®å®šè¦åŒæ­¥çš„è¡¨ID
        sync_table_ids = table_ids if table_ids else []
        if not sync_table_ids:
            # ä½¿ç”¨é…ç½®çš„è¡¨ID
            table_ids_str = config.API_TABLE_IDS.strip()
            if table_ids_str:
                sync_table_ids = [int(tid.strip()) for tid in table_ids_str.split(',') if tid.strip()]
        
        if not sync_table_ids:
            raise HTTPException(
                status_code=400,
                detail="æœªæä¾›è¡¨IDä¸”æœªé…ç½®API_TABLE_IDSç¯å¢ƒå˜é‡"
            )
        
        logger.info(f"æ‰‹åŠ¨è§¦å‘å…ƒæ•°æ®åŒæ­¥ï¼Œè¡¨ID: {sync_table_ids}")
        
        # ä»APIåŠ è½½å…ƒæ•°æ®
        loader = MetadataLoader(jwt=jwt)
        fields = loader.load_from_api(sync_table_ids)
        
        if not fields:
            raise HTTPException(
                status_code=500,
                detail="ä»APIåŠ è½½å…ƒæ•°æ®å¤±è´¥æˆ–è¿”å›ä¸ºç©º"
            )
        
        # æ›´æ–°ESç´¢å¼•
        if not searcher.es_engine:
            raise HTTPException(status_code=500, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if not searcher.es_engine.index_exists():
            searcher.es_engine.create_index(force=True)
        
        # æ‰¹é‡ç´¢å¼•
        index_result = searcher.es_engine.bulk_index_fields(fields)
        
        # é‡æ–°åˆå§‹åŒ–æœç´¢å¼•æ“
        if searcher.ac_matcher:
            searcher.ac_matcher.initialize(fields)
        if searcher.similarity_matcher:
            searcher.similarity_matcher.initialize(fields)
        searcher.fields_data = fields
        
        return {
            "success": True,
            "message": f"æˆåŠŸåŒæ­¥ {index_result.get('success', 0)} ä¸ªå­—æ®µ",
            "stats": {
                "fields_loaded": len(fields),
                "fields_indexed": index_result.get('success', 0),
                "table_ids": sync_table_ids
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨åŒæ­¥å…ƒæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


@router.post("/sync/metrics",
             summary="æ‰‹åŠ¨è§¦å‘æŒ‡æ ‡åŒæ­¥",
             description="ä»APIæ‰‹åŠ¨åŒæ­¥æŒ‡æ ‡åˆ°Elasticsearch")
async def sync_metrics_from_api(
    jwt: Optional[str] = Query(None, description="JWTè®¤è¯tokenï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„JWT"),
    ids: Optional[str] = Query(None, description="æŒ‡æ ‡IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚171,172ï¼‰ï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–åŠ è½½æ‰€æœ‰"),
    force: bool = Query(True, description="æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆé»˜è®¤Trueï¼‰"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æ‰‹åŠ¨ä»APIåŒæ­¥æŒ‡æ ‡åˆ°ES
    
    ## æ³¨æ„
    - åŒæ­¥è¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆéœ€è¦è·å–æ¯ä¸ªæŒ‡æ ‡çš„è¯¦æƒ…ï¼‰
    - ä½¿ç”¨å¹¶è¡Œæ–¹å¼åŠ é€ŸåŒæ­¥
    - idså‚æ•°ç¤ºä¾‹: "171,172" æˆ– "171,172,173"
    """
    try:
        from indexing.data_loader import MetricLoader
        
        # ä»APIåŠ è½½æŒ‡æ ‡
        loader = MetricLoader(jwt=jwt)
        metrics = loader.load_from_api(max_workers=10, ids=ids)
        
        if not metrics:
            raise HTTPException(
                status_code=500,
                detail="ä»APIåŠ è½½æŒ‡æ ‡å¤±è´¥æˆ–è¿”å›ä¸ºç©º"
            )
        
        # æ›´æ–°ESç´¢å¼•
        if not searcher.es_engine:
            raise HTTPException(status_code=500, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        # å¼ºåˆ¶é‡å»ºæŒ‡æ ‡ç´¢å¼•ï¼ˆæ‰‹åŠ¨åŒæ­¥æ—¶åˆ é™¤æ—§ç´¢å¼•ï¼Œåˆ›å»ºæ–°ç´¢å¼•ï¼‰
        logger.info("æ‰‹åŠ¨åŒæ­¥ï¼šåˆ é™¤æ—§æŒ‡æ ‡ç´¢å¼•å¹¶é‡å»º...")
        searcher.es_engine.create_metric_index(force=force)
        
        # æ‰¹é‡ç´¢å¼•
        success = searcher.es_engine.bulk_index_metrics(metrics)
        
        if success:
            return {
                "success": True,
                "message": f"æˆåŠŸåŒæ­¥ {len(metrics)} ä¸ªæŒ‡æ ‡",
                "stats": {
                    "metrics_loaded": len(metrics),
                    "metrics_indexed": len(metrics)
                }
            }
        else:
            raise HTTPException(status_code=500, detail="æŒ‡æ ‡ç´¢å¼•å¤±è´¥")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨åŒæ­¥æŒ‡æ ‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


@router.post("/sync/dimension-values",
             summary="æ‰‹åŠ¨è§¦å‘ç»´åº¦å€¼åŒæ­¥",
             description="ä»æ•°æ®åº“æå–ç»´åº¦å€¼å¹¶åŒæ­¥åˆ°Elasticsearch")
async def sync_dimension_values(
    jwt: Optional[str] = Query(None, description="JWTè®¤è¯tokenï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„JWT"),
    force: bool = Query(True, description="æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆé»˜è®¤Trueï¼‰"),
    searcher: HybridSearcher = Depends(get_hybrid_searcher)
):
    """
    æ‰‹åŠ¨ä»æ•°æ®åº“æå–ç»´åº¦å€¼å¹¶åŒæ­¥åˆ°ES
    
    ## æ³¨æ„
    - éœ€è¦å…ˆç¡®ä¿å…ƒæ•°æ®å·²åŒæ­¥ï¼ˆéœ€è¦çŸ¥é“å“ªäº›å­—æ®µæ˜¯ç»´åº¦å­—æ®µï¼‰
    - ä¼šä»æ•°æ®åº“ä¸­æå–æ‰€æœ‰ç»´åº¦å­—æ®µçš„å”¯ä¸€å€¼
    - é»˜è®¤å¼ºåˆ¶é‡å»ºç´¢å¼•ä»¥ç¡®ä¿æ•°æ®å¹²å‡€
    """
    try:
        from indexing.data_loader import MetadataLoader
        from indexing.dimension_extractor import EnhancedDimensionExtractor
        
        # åŠ è½½å…ƒæ•°æ®ï¼ˆè·å–ç»´åº¦å­—æ®µä¿¡æ¯ï¼‰
        loader = MetadataLoader(jwt=jwt)
        fields = loader.load()
        
        if not fields:
            raise HTTPException(
                status_code=500,
                detail="åŠ è½½å…ƒæ•°æ®å¤±è´¥æˆ–è¿”å›ä¸ºç©º"
            )
        
        # æ›´æ–°ESç´¢å¼•
        if not searcher.es_engine:
            raise HTTPException(status_code=500, detail="Elasticsearchå¼•æ“ä¸å¯ç”¨")
        
        # å¼ºåˆ¶é‡å»ºç»´åº¦å€¼ç´¢å¼•
        logger.info("æ‰‹åŠ¨åŒæ­¥ï¼šåˆ é™¤æ—§ç»´åº¦å€¼ç´¢å¼•å¹¶é‡å»º...")
        dimension_index_created = searcher.es_engine.create_dimension_values_index(force)
        
        if not dimension_index_created:
            raise HTTPException(status_code=500, detail="ç»´åº¦å€¼ç´¢å¼•åˆ›å»ºå¤±è´¥")
        
        # æå–ç»´åº¦å€¼
        extractor = EnhancedDimensionExtractor()
        dimension_values = extractor.extract_all_dimension_values(fields)
        extractor.close_connections()
        
        if not dimension_values:
            return {
                "success": True,
                "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦æå–çš„ç»´åº¦å€¼",
                "stats": {
                    "dimension_values_extracted": 0,
                    "dimension_values_indexed": 0
                }
            }
        
        # æ‰¹é‡ç´¢å¼•ç»´åº¦å€¼
        index_result = searcher.es_engine.bulk_index_dimension_values(dimension_values)
        
        return {
            "success": True,
            "message": f"æˆåŠŸæå–å¹¶ç´¢å¼• {index_result.get('success', 0)} ä¸ªç»´åº¦å€¼",
            "stats": {
                "dimension_values_extracted": len(dimension_values),
                "dimension_values_indexed": index_result.get('success', 0),
                "dimension_values_failed": index_result.get('failed', 0)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨åŒæ­¥ç»´åº¦å€¼å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


@router.get("/sync/status",
            summary="è·å–åŒæ­¥çŠ¶æ€",
            description="è·å–æ•°æ®åŒæ­¥è°ƒåº¦å™¨çš„çŠ¶æ€å’Œæœ€åä¸€æ¬¡åŒæ­¥çš„ç»“æœ")
async def get_sync_status():
    """
    è·å–åŒæ­¥çŠ¶æ€
    
    ## è¿”å›ä¿¡æ¯
    - enabled: æ˜¯å¦å¯ç”¨åŒæ­¥
    - interval_hours: åŒæ­¥é—´éš”ï¼ˆå°æ—¶ï¼‰
    - table_ids: é…ç½®çš„è¡¨IDåˆ—è¡¨
    - is_syncing: æ˜¯å¦æ­£åœ¨åŒæ­¥
    - last_sync_time: æœ€åä¸€æ¬¡åŒæ­¥æ—¶é—´
    - last_sync_status: æœ€åä¸€æ¬¡åŒæ­¥çš„è¯¦ç»†çŠ¶æ€
    - scheduler_running: è°ƒåº¦å™¨æ˜¯å¦è¿è¡Œä¸­
    """
    try:
        global _data_sync_scheduler
        
        if _data_sync_scheduler is None:
            from core.config import config
            return {
                "enabled": config.API_SYNC_ENABLED,
                "message": "æ•°æ®åŒæ­¥è°ƒåº¦å™¨æœªåˆå§‹åŒ–",
                "scheduler_running": False
            }
        
        status = _data_sync_scheduler.get_status()
        return status
        
    except Exception as e:
        logger.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")


# ==================== ç»¼åˆåˆ†æAPI ====================

@router.post("/comprehensive-analysis",
             response_model=ComprehensiveAnalysisResponse,
             summary="ç»¼åˆæ•°æ®åˆ†æ",
             description="å¯¹æ•°æ®æ‰§è¡Œç»¼åˆåˆ†æï¼ŒåŒ…æ‹¬åŸºç¡€ç»Ÿè®¡ã€è¶‹åŠ¿ã€åˆ†ç»„èšåˆç­‰")
async def comprehensive_analysis_api(request: ComprehensiveAnalysisRequest):
    """
    ç»¼åˆæ•°æ®åˆ†ææ¥å£
    
    ## åŠŸèƒ½è¯´æ˜
    å¯¹æä¾›çš„æ•°æ®æ‰§è¡Œå…¨æ–¹ä½åˆ†æï¼ŒåŒ…æ‹¬ï¼š
    - âœ… åŸºç¡€ç»Ÿè®¡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ï¼‰
    - âœ… å››åˆ†ä½æ•°åˆ†æ
    - âœ… åˆ†å¸ƒåˆ†æï¼ˆç›´æ–¹å›¾ã€ç®±çº¿å›¾ï¼‰
    - âœ… è¶‹åŠ¿åˆ†æï¼ˆçº¿æ€§å›å½’ã€RÂ²æ‹Ÿåˆåº¦ï¼‰
    - âœ… åˆ†ç»„èšåˆï¼ˆæŒ‰ç»´åº¦åˆ†ç»„ç»Ÿè®¡ï¼‰
    - âœ… åˆ†ç»„è¶‹åŠ¿ï¼ˆå„åˆ†ç»„çš„è¶‹åŠ¿åˆ†æï¼‰
    - âŒ å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆå·²è¿‡æ»¤ï¼‰
    - âŒ å¯¹æ¯”åˆ†æï¼ˆå·²è¿‡æ»¤ï¼‰
    
    ## è¯·æ±‚å‚æ•°
    ```json
    {
        "metric_api_address": "http://api.example.com",
        "JWT": "Bearer xxx",
        "data": {
            "rows": [{"æ—¥æœŸ": "2024-01-01", "é”€å”®é¢": 1000, "åŒºåŸŸ": "åä¸œ"}],
            "target_columns": ["é”€å”®é¢"],
            "date_column": "æ—¥æœŸ",
            "group_by": ["åŒºåŸŸ"],
            "filter_obj": {}
        }
    }
    ```
    
    ## å“åº”ç¤ºä¾‹
    ```json
    {
        "success": true,
        "comprehensive_result": {
            "é”€å”®é¢": {
                "basic_stats": {...},
                "quartiles": {...},
                "trend": {...},
                "groupby_agg": [...],
                "group_trend": [...]
            }
        },
        "took": 1234
    }
    ```
    
    ## æ³¨æ„äº‹é¡¹
    - å¦‚æœ `target_columns` ä¸ºç©ºï¼Œä¼šè‡ªåŠ¨æ¨æ–­æ•°å€¼åˆ—
    - å¦‚æœ `date_column` å­˜åœ¨ä¸”æ•°æ®ä¸­æœ‰æ—¥æœŸå­—æ®µï¼Œä¼šæ‰§è¡Œè¶‹åŠ¿åˆ†æ
    - å¦‚æœ `group_by` éç©ºï¼Œä¼šæ‰§è¡Œåˆ†ç»„èšåˆå’Œåˆ†ç»„è¶‹åŠ¿åˆ†æ
    - åˆ†ç»„å­—æ®µå’Œæ—¥æœŸå­—æ®µä¸èƒ½ç›¸åŒï¼ˆä¼šå¯¼è‡´åˆ†ç»„è¶‹åŠ¿åˆ†æå¤±è´¥ï¼‰
    """
    import time
    from datetime import datetime
    
    start_time = time.time()
    
    try:
        # å¯¼å…¥ cal.py ä¸­çš„ comprehensive_analysis å‡½æ•°
        try:
            from indexing.cal import comprehensive_analysis
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥ comprehensive_analysis å‡½æ•°: {e}")
            return ComprehensiveAnalysisResponse(
                success=False,
                error="æœåŠ¡å™¨é…ç½®é”™è¯¯ï¼šæ— æ³•åŠ è½½åˆ†ææ¨¡å—",
                took=0
            )
        
        # éªŒè¯å¿…å¡«å‚æ•°
        if not request.data: 
            return ComprehensiveAnalysisResponse(
                success=False,
                error="ç¼ºå°‘å¿…å¡«å‚æ•°: data",
                took=0
            )
        
        # éªŒè¯åˆ†ç»„è¶‹åŠ¿é…ç½®
        group_by = request.data.get("group_by", [])
        date_column = request.data.get("date_column", "")
        
        if group_by and date_column and date_column in group_by:
            return ComprehensiveAnalysisResponse(
                success=False,
                error=f"é…ç½®é”™è¯¯ï¼šåˆ†ç»„å­—æ®µ {group_by} åŒ…å«æ—¥æœŸå­—æ®µ '{date_column}'ã€‚"
                      f"è¿™ä¼šå¯¼è‡´æ¯ä¸ªç»„å†…åªæœ‰ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æã€‚"
                      f"å»ºè®®ä» group_by ä¸­ç§»é™¤æ—¥æœŸå­—æ®µã€‚",
                took=0
            )
        
        # è°ƒç”¨ç»¼åˆåˆ†æå‡½æ•°
        logger.info(f"å¼€å§‹æ‰§è¡Œç»¼åˆåˆ†æ: target_columns={request.data.get('target_columns')}, "
                   f"date_column={date_column}, group_by={group_by}")
        
        result = comprehensive_analysis(
            metric_api_address=request.metric_api_address,
            JWT=request.JWT,
            data=request.data
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in result:
            logger.warning(f"ç»¼åˆåˆ†æè¿”å›é”™è¯¯: {result['error']}")
            return ComprehensiveAnalysisResponse(
                success=False,
                error=result["error"],
                took=int((time.time() - start_time) * 1000)
            )
        
        # è¿‡æ»¤ç»“æœï¼šç§»é™¤å¼‚å¸¸å€¼æ£€æµ‹å’Œå¯¹æ¯”åˆ†æ
        filtered_result = _filter_comprehensive_result(result)
        
        took_ms = int((time.time() - start_time) * 1000)
        logger.info(f"ç»¼åˆåˆ†æå®Œæˆï¼Œè€—æ—¶ {took_ms}ms")
        
        return ComprehensiveAnalysisResponse(
            success=True,
            comprehensive_result=filtered_result.get("comprehensive_result"),
            took=took_ms
        )
        
    except Exception as e:
        logger.error(f"ç»¼åˆåˆ†æå¤±è´¥: {e}", exc_info=True)
        took_ms = int((time.time() - start_time) * 1000)
        return ComprehensiveAnalysisResponse(
            success=False,
            error=f"åˆ†æå¤±è´¥: {str(e)}",
            took=took_ms
        )


def _filter_comprehensive_result(result: dict) -> dict:
    """
    è¿‡æ»¤ç»¼åˆåˆ†æç»“æœï¼Œç§»é™¤å¼‚å¸¸å€¼æ£€æµ‹å’Œå¯¹æ¯”åˆ†æ
    ä¿ç•™ï¼šåŸºç¡€ç»Ÿè®¡ã€å››åˆ†ä½æ•°ã€åˆ†å¸ƒã€è¶‹åŠ¿ã€åˆ†ç»„èšåˆã€åˆ†ç»„è¶‹åŠ¿
    """
    if "comprehensive_result" not in result:
        return result
    
    filtered = {"comprehensive_result": {}}
    
    for column_name, column_data in result["comprehensive_result"].items():
        filtered_column = {}
        
        # ä¿ç•™éœ€è¦çš„å­—æ®µ
        keep_fields = ["basic_stats", "quartiles", "distribution", "trend", "groupby_agg", "group_trend"]
        
        for field in keep_fields:
            if field in column_data:
                filtered_column[field] = column_data[field]
        
        # ç§»é™¤çš„å­—æ®µï¼šoutliers, compare
        
        filtered["comprehensive_result"][column_name] = filtered_column
    
    return filtered 